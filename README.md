Huffman Encoding - Greedy Algorithm

In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".

We have reduced the number of bits used by the text document by calculating the number of occurances of each ASCII(American Standard Code for Information Interchange) value.

The compression Ratio  has to be as huge as possible so that the compression becomes more efficient. 

A Document of 87.2 MB was reduced to 50.6 MB with the Compression Ratio - 2.695649233727809. 
